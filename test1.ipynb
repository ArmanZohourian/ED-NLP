{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:06:58.644291Z",
     "start_time": "2025-06-24T19:06:57.724402Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q transformers datasets accelerate scikit-learn torch",
   "id": "684be84db8d073db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "235bd4479db5944f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T19:06:58.675631Z",
     "start_time": "2025-06-24T19:06:58.672227Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set up dataset",
   "id": "2b64bca23bc437d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:06:58.704136Z",
     "start_time": "2025-06-24T19:06:58.689864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_PATH = \"track-a.csv\"\n",
    "label_cols = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "id": "b2d3432aeb0a5fd6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HuggingFace Dataset & tokenisation",
   "id": "be267786fd96164c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:06:59.413961Z",
     "start_time": "2025-06-24T19:06:58.717053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer   = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_ds = datasets.Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = datasets.Dataset.from_pandas(test_df .reset_index(drop=True))\n",
    "ds       = datasets.DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
    "\n",
    "# attach list-of-ints label field expected by HF\n",
    "def add_labels(example):\n",
    "    example[\"labels\"] = [float(example[c]) for c in label_cols]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(lambda x: tokenizer(x['text']), batched=True, remove_columns=[\"text\", \"id\"])\n",
    "ds = ds.map(add_labels)\n",
    "ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "id": "58d347d43f709c2f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2214/2214 [00:00<00:00, 55664.59 examples/s]\n",
      "Map: 100%|██████████| 554/554 [00:00<00:00, 63713.86 examples/s]\n",
      "Map: 100%|██████████| 2214/2214 [00:00<00:00, 25674.23 examples/s]\n",
      "Map: 100%|██████████| 554/554 [00:00<00:00, 23239.23 examples/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4 Model & Trainer",
   "id": "fc18799827c1e34e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:10:58.550885Z",
     "start_time": "2025-06-24T19:06:59.435116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_labels = len(label_cols)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            problem_type=\"multi_label_classification\")\n",
    "\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    return {\n",
    "        \"micro_f1\": sklearn.metrics.f1_score(labels, preds, average=\"micro\"),\n",
    "        \"macro_f1\": sklearn.metrics.f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"sentiment_model\",\n",
    "    logging_steps=25,\n",
    "    dataloader_pin_memory=False, # otherwise, this warning on MacBook M1: \"UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\"\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args,\n",
    "                  train_dataset=ds[\"train\"],\n",
    "                  eval_dataset=ds[\"test\"],\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nHeld-out metrics:\", metrics)"
   ],
   "id": "fb7267e930a461c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/25/22j442jd1kbbtszy07f7l15h0000gn/T/ipykernel_22116/148529727.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=args,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='831' max='831' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [831/831 03:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.472100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.424100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Held-out metrics: {'eval_loss': 0.41554585099220276, 'eval_micro_f1': 0.7070101857399641, 'eval_macro_f1': 0.6443928307047027, 'eval_runtime': 3.1458, 'eval_samples_per_second': 176.108, 'eval_steps_per_second': 22.252, 'epoch': 3.0}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5: Report",
   "id": "e0c90c319eb4099a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:11:03.100311Z",
     "start_time": "2025-06-24T19:10:58.587022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred_logits = trainer.predict(ds[\"test\"]).predictions\n",
    "y_pred = (sigmoid(pred_logits) > 0.5).astype(int)\n",
    "y_true = np.vstack(ds[\"test\"][\"labels\"])\n",
    "\n",
    "print(\"\\nClassification report\")\n",
    "print(sklearn.metrics.classification_report(y_true, y_pred,\n",
    "                            target_names=label_cols))"
   ],
   "id": "1ae00fa4cf616869",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.70      0.32      0.44        72\n",
      "        fear       0.77      0.83      0.80       330\n",
      "         joy       0.69      0.57      0.63       115\n",
      "     sadness       0.66      0.63      0.65       167\n",
      "    surprise       0.76      0.68      0.71       179\n",
      "\n",
      "   micro avg       0.73      0.68      0.71       863\n",
      "   macro avg       0.71      0.61      0.64       863\n",
      "weighted avg       0.73      0.68      0.70       863\n",
      " samples avg       0.65      0.63      0.61       863\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moritzgross/PycharmProjects/ED-NLP/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/moritzgross/PycharmProjects/ED-NLP/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/moritzgross/PycharmProjects/ED-NLP/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:11:03.117126Z",
     "start_time": "2025-06-24T19:11:03.113647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Equal fields: {np.mean(y_pred == y_true):.2f}\")\n",
    "print(f\"Equal rows: {np.mean((y_pred == y_true).all(axis=1)):.2f}\")"
   ],
   "id": "a868e9f772323049",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal fields: 0.82\n",
      "Equal rows: 0.41\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
